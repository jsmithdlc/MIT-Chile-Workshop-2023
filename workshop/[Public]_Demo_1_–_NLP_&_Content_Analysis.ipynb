{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6rjRQDH93cP"
      },
      "source": [
        "# Workshop: NLP & Content Analysis\n",
        "\n",
        "This workshop session will provide a hands-on experience for you to learn modern \"out-of-the-box\" natural language processing techniques that will allow you to design systematic methods to analize text.\n",
        "\n",
        "**During this lab you will**:\n",
        "\n",
        "- Get hands-on experience through the following modules:\n",
        "  - Sentiment lexicon\n",
        "  - Valence recognition\n",
        "  - Perspective API\n",
        "  - Word embeddings\n",
        "  - Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWXr0TbU_oGF"
      },
      "source": [
        "# 0. Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please make sure to change the runtime of this Colab notebook to use GPUs.** Select 'Runtime' --> 'Change runtime type' --> select 'GPU' for the Hardware accelerator. GPUs are optimized for graphics-related computations, which involve many matrix multiplications. It turns out that matrix multiplications are similarly pervasive in modern machine learning (i.e. deep neural networks), and GPUs can greatly increase the speed of using these models."
      ],
      "metadata": {
        "id": "AszsjdU0dpuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uk2nO0C5CzTv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gensim\n",
        "!pip install sentence-transformers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 0)\n",
        "pd.set_option('display.max_columns', 999)\n",
        "\n",
        "import itertools\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import gensim\n",
        "import scipy\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzVdhct0tzSN"
      },
      "source": [
        "#### Download data and models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJKeFQjRIEWm"
      },
      "source": [
        "The following code downloads the data and our toolkit for exploring this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JRi-5A6OoFaq",
        "outputId": "0aa37e98-1874-4d42-8841-0f180619ed7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ogRFqUjIUqBxIUkGX6z8DobOWonOJp_u\n",
            "To: /content/reddit_workshop_sample.csv\n",
            "\r  0% 0.00/788k [00:00<?, ?B/s]\r100% 788k/788k [00:00<00:00, 160MB/s]\n",
            "--2023-01-10 15:20:01--  https://public-thought.media.mit.edu/static/ccc_toolkit_v_21_0.py\n",
            "Resolving public-thought.media.mit.edu (public-thought.media.mit.edu)... 18.27.78.114\n",
            "Connecting to public-thought.media.mit.edu (public-thought.media.mit.edu)|18.27.78.114|:443... connected.\n",
            "WARNING: cannot verify public-thought.media.mit.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
            "  Issued certificate has expired.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13578 (13K) [application/octet-stream]\n",
            "Saving to: ‘ccc_toolkit_v_21_0.py’\n",
            "\n",
            "ccc_toolkit_v_21_0. 100%[===================>]  13.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-10 15:20:02 (247 MB/s) - ‘ccc_toolkit_v_21_0.py’ saved [13578/13578]\n",
            "\n",
            "--2023-01-10 15:20:02--  https://saifmohammad.com/WebDocs/Lexicons/NRC-Emotion-Lexicon.zip\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25878449 (25M) [application/zip]\n",
            "Saving to: ‘NRC-Emotion-Lexicon.zip’\n",
            "\n",
            "NRC-Emotion-Lexicon 100%[===================>]  24.68M  5.78MB/s    in 4.3s    \n",
            "\n",
            "2023-01-10 15:20:07 (5.78 MB/s) - ‘NRC-Emotion-Lexicon.zip’ saved [25878449/25878449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get reddit sample\n",
        "!gdown https://drive.google.com/uc?id=1ogRFqUjIUqBxIUkGX6z8DobOWonOJp_u\n",
        "\n",
        "# get backend toolkit code\n",
        "!wget https://public-thought.media.mit.edu/static/ccc_toolkit_v_21_0.py --no-check-certificate\n",
        "\n",
        "# get emotions lexicon\n",
        "!wget https://saifmohammad.com/WebDocs/Lexicons/NRC-Emotion-Lexicon.zip\n",
        "!unzip -qq NRC-Emotion-Lexicon.zip\n",
        "\n",
        "# Download and load data (this is going to take a couple of minutes.)\n",
        "from ccc_toolkit_v_21_0 import (get_clusters,\n",
        "                          plot_tsne_viz,\n",
        "                          set_replicable_results,\n",
        "                          run_clustering,\n",
        "                          retrieve,\n",
        "                          SentenceTransformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dataset"
      ],
      "metadata": {
        "id": "Baxg7zEObPVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reddit Moderated Comments (Sample)\n",
        "\n",
        "Selected sample of comments that have been removed by Reddit moderators.\n"
      ],
      "metadata": {
        "id": "tgLTES_uQeUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "reddit_data = pd.read_csv('reddit_workshop_sample.csv')"
      ],
      "metadata": {
        "id": "N8_6rSnWQ1WH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.columns"
      ],
      "metadata": {
        "id": "HDoT0jwvIUCy",
        "outputId": "eea5ec9b-24ee-42bd-b488-9a5ec3b96a11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'subreddit', 'subreddit description', 'violation reason',\n",
              "       'moderator comment', 'comment link', 'internal id'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.shape"
      ],
      "metadata": {
        "id": "GS5H9MRgQ5QY",
        "outputId": "b69f3142-5e4a-4e17-c884-4e095a8b8651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(659, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data[:2]"
      ],
      "metadata": {
        "id": "xB2Roy3gRJzo",
        "outputId": "22ca259b-3757-4587-fd42-3fc12d6f29c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
              "0  Oh, himbs is so sad that I won't agree to his weird need to control other people.\\n\\nSad bro, real sad. I picture you at a party, lecturing at some hot thing as they frantically search for a way to escape the awful drudgery of listening to you.                                                                                                        \n",
              "1  Apparently his legs are too short to walk him back to the counter because he is a (baby). \\n(Hopefully I won’t get banned for that)\\nHonestly, WHY CANT HE ASK LIKE AN ADULT IF HE WANTS SOMETHING??\\n\\nAnd then he doubles down saying he shouldn’t ‘have to’ act like an adult because he should be able to take anything he wants from her?!\\n\\nWHAT?!   \n",
              "\n",
              "         subreddit  \\\n",
              "0  r/AmItheAsshole   \n",
              "1  r/AmItheAsshole   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                              subreddit description  \\\n",
              "0  A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that's been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you're right, or you're the asshole. \\n\\nSee our ~~*Best Of*~~ \"Most Controversial\" at /r/AITAFiltered!   \n",
              "1  A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that's been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you're right, or you're the asshole. \\n\\nSee our ~~*Best Of*~~ \"Most Controversial\" at /r/AITAFiltered!   \n",
              "\n",
              "                      violation reason  \\\n",
              "0  incivility, overly cruel or hostile   \n",
              "1  incivility, overly cruel or hostile   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                   moderator comment  \\\n",
              "0  Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**   \n",
              "1  Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**   \n",
              "\n",
              "                                                                                  comment link  \\\n",
              "0  /r/AmItheAsshole/comments/vuh8j2/aita_for_not_taking_my_kid_to_a_class_that_his/ifgjrwt/      \n",
              "1  /r/AmItheAsshole/comments/vj194d/aita_for_not_giving_my_mayonnaise_to_my_boyfriend/idgw640/   \n",
              "\n",
              "   internal id  \n",
              "0  6869         \n",
              "1  5938         "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df986bb7-c7ed-43d2-9693-9de763077340\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>subreddit description</th>\n",
              "      <th>violation reason</th>\n",
              "      <th>moderator comment</th>\n",
              "      <th>comment link</th>\n",
              "      <th>internal id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oh, himbs is so sad that I won't agree to his weird need to control other people.\\n\\nSad bro, real sad. I picture you at a party, lecturing at some hot thing as they frantically search for a way to escape the awful drudgery of listening to you.</td>\n",
              "      <td>r/AmItheAsshole</td>\n",
              "      <td>A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that's been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you're right, or you're the asshole. \\n\\nSee our ~~*Best Of*~~ \"Most Controversial\" at /r/AITAFiltered!</td>\n",
              "      <td>incivility, overly cruel or hostile</td>\n",
              "      <td>Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**</td>\n",
              "      <td>/r/AmItheAsshole/comments/vuh8j2/aita_for_not_taking_my_kid_to_a_class_that_his/ifgjrwt/</td>\n",
              "      <td>6869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apparently his legs are too short to walk him back to the counter because he is a (baby). \\n(Hopefully I won’t get banned for that)\\nHonestly, WHY CANT HE ASK LIKE AN ADULT IF HE WANTS SOMETHING??\\n\\nAnd then he doubles down saying he shouldn’t ‘have to’ act like an adult because he should be able to take anything he wants from her?!\\n\\nWHAT?!</td>\n",
              "      <td>r/AmItheAsshole</td>\n",
              "      <td>A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that's been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you're right, or you're the asshole. \\n\\nSee our ~~*Best Of*~~ \"Most Controversial\" at /r/AITAFiltered!</td>\n",
              "      <td>incivility, overly cruel or hostile</td>\n",
              "      <td>Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**</td>\n",
              "      <td>/r/AmItheAsshole/comments/vj194d/aita_for_not_giving_my_mayonnaise_to_my_boyfriend/idgw640/</td>\n",
              "      <td>5938</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df986bb7-c7ed-43d2-9693-9de763077340')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df986bb7-c7ed-43d2-9693-9de763077340 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df986bb7-c7ed-43d2-9693-9de763077340');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.subreddit.value_counts()"
      ],
      "metadata": {
        "id": "-mXhV9gJTzjE",
        "outputId": "5757b2dd-a6fc-476f-c73a-c86c33af0a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "r/AmItheAsshole         125\n",
              "r/changemyview          105\n",
              "r/collapse              45 \n",
              "r/texas                 44 \n",
              "r/DestinyTheGame        40 \n",
              "r/Christianity          37 \n",
              "r/NintendoSwitch        37 \n",
              "r/Israel                29 \n",
              "r/UFOs                  25 \n",
              "r/Fantasy               24 \n",
              "r/Dallas                22 \n",
              "r/povertyfinance        14 \n",
              "r/explainlikeimfive     14 \n",
              "r/mormon                12 \n",
              "r/RPClipsGTA            10 \n",
              "r/legaladvice           10 \n",
              "r/buildapc              9  \n",
              "r/AskTrumpSupporters    9  \n",
              "r/doctorwho             5  \n",
              "r/rpg                   5  \n",
              "r/ShingekiNoKyojin      5  \n",
              "r/sex                   5  \n",
              "r/MMORPG                5  \n",
              "r/China                 5  \n",
              "r/liberalgunowners      5  \n",
              "r/medicine              5  \n",
              "r/syriancivilwar        4  \n",
              "r/ExperiencedDevs       4  \n",
              "Name: subreddit, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data['violation reason'].value_counts()"
      ],
      "metadata": {
        "id": "6ENJ_Rw3IkIp",
        "outputId": "01176418-d756-41e5-c18b-436134d51405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "incivility, overly cruel or hostile                                       125\n",
              "rule b -d party/devils advocate/soapboxing (ops only)                     105\n",
              "be respectful to others                                                   54 \n",
              "be friendly                                                               44 \n",
              "no uncivil behavior, witchhunting, etc                                    40 \n",
              "harassment, pestering, insistence upon debate, etc                        37 \n",
              "no hate-speech, personal attacks or harassment                            37 \n",
              "post in a civilized manner                                                29 \n",
              "follow the standards of civility                                          25 \n",
              "be kind                                                                   24 \n",
              "civility                                                                  14 \n",
              "uncivil behavior                                                          14 \n",
              "being uncivil                                                             12 \n",
              "not civil                                                                 12 \n",
              "uncivil                                                                   10 \n",
              "personal attack                                                           10 \n",
              "be civil and sincere in all interactions and assume the same of others    9  \n",
              "discriminatory language                                                   6  \n",
              "disrespectful/objectifying/not constructive                               5  \n",
              "this person is being excessively incivil                                  5  \n",
              "unprofessional, rude, or inappropriate behaviour                          5  \n",
              "be nice                                                                   5  \n",
              "personal insults, homophobia, sexism, racism, etc                         5  \n",
              "general conduct                                                           5  \n",
              "toxic behavior                                                            5  \n",
              "r be respectful                                                           5  \n",
              "no disrespectful language or conduct                                      4  \n",
              "soapboxing, arguing                                                       2  \n",
              "(r racist/group derogatory                                                2  \n",
              "(r uncivil/disrespectful/ad hominem                                       2  \n",
              "trolling                                                                  2  \n",
              "Name: violation reason, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data[['moderator comment']]"
      ],
      "metadata": {
        "id": "Ye-sBsRhT4rb",
        "outputId": "65f1d5fb-5648-490d-e3f2-deaf3cb0502c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        moderator comment\n",
              "0    Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "1    Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "2    Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "3    Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "4    Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "..                                                                                                                                                                                                                                                                                                                                                                                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "654  Warning - Removed for Rule 1. Discuss in good faith, please. No accusations of lying.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
              "655  **Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;subject=Please%20review%20my%20thread?&amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/ufsm2y/-/i6vmwdc/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.\n",
              "656  **Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;subject=Please%20review%20my%20thread?&amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/vlw94u/-/ie1fo55/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.\n",
              "657  **Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;subject=Please%20review%20my%20thread?&amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/uykuul/-/ia5hn06/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.\n",
              "658  **Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;subject=Please%20review%20my%20thread?&amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/v922da/-/ibucs06/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.\n",
              "\n",
              "[659 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8e5dcb3-4798-434d-9013-3327525c2357\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>moderator comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Your comment has been removed because it violates rule 1: [Be Civil](https://www.reddit.com/r/AmItheAsshole/about/rules/). Further incidents may result in a ban.\\r\\n\\r\\n[\"Why do I have to be civil in a sub about assholes?\"](https://www.reddit.com/r/AmItheAsshole/wiki/faq)\\r\\n\\r\\n**[Message the mods](https://www.reddit.com/message/compose?to=/r/AmItheAsshole) if you have any questions or concerns.**</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>Warning - Removed for Rule 1. Discuss in good faith, please. No accusations of lying.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>**Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;amp;subject=Please%20review%20my%20thread?&amp;amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/ufsm2y/-/i6vmwdc/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>**Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;amp;subject=Please%20review%20my%20thread?&amp;amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/vlw94u/-/ie1fo55/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>**Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;amp;subject=Please%20review%20my%20thread?&amp;amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/uykuul/-/ia5hn06/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>**Please read this entire message**\\r\\n\\r\\n---\\r\\n\\r\\nYour comment has been removed for the following reason(s):\\r\\n\\r\\n* Rule #1 of ELI5 is to *be nice*. Breaking Rule 1 is not tolerated.\\r\\n\\r\\n\\r\\n\\r\\n---\\r\\nIf you would like this removal reviewed, please read the [detailed rules](https://www.reddit.com/r/explainlikeimfive/wiki/detailed_rules) first. **If you believe this comment was removed erroneously**, please [use this form](https://old.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive&amp;amp;subject=Please%20review%20my%20thread?&amp;amp;message=Link:%20https://www.reddit.com/r/explainlikeimfive/comments/v922da/-/ibucs06/%0A%0APlease%20answer%20the%20following%203%20questions:%0A%0A1.%20The%20concept%20I%20want%20explained:%0A%0A2.%20List%20the%20search%20terms%20you%20used%20to%20look%20for%20past%20posts%20on%20ELI5:%0A%0A3.%20How%20is%20this%20post%20unique:) and we will review your submission.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>659 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8e5dcb3-4798-434d-9013-3327525c2357')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8e5dcb3-4798-434d-9013-3327525c2357 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8e5dcb3-4798-434d-9013-3327525c2357');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxRb97zCboQo"
      },
      "source": [
        "# 2. Sentiment Lexicons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CYKnvorsLMP"
      },
      "source": [
        "We might be interested in when analyzing the would be how emotions expressed in these comments.\n",
        "\n",
        "Natural language processing has some techniques we can use to understand the emotional arc of conversations!\n",
        "\n",
        "This field of NLP is called \"sentiment analysis.\" \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQzqIG_khfId"
      },
      "source": [
        "We will use a list of words called a lexicon to make an emotion classifier, a model which will tell us whether a sentence is generally positive or negative emotion in nature.\n",
        "\n",
        "We can make this simple classifier using a sentiment and emotion lexicon called [Emolex](http://saifmohammad.com/WebPages/lexicons.html).\n",
        "\n",
        "Each word in the sentiment lexicon is tagged with either 'positive', 'negative', or even both in rare cases. Each word in the emotion lexicon is tagged with one of the 8 emotions according to Plutchik's wheel of emotions -- joy, trust, fear, surprise, sadness, anticipation, anger, and disgust. This is, of course, just one theory of emotion. Other lexicons are available within Emolex, labeling words from different domains along different dimensions (e.g., valence, arousal, dominance).\n",
        "\n",
        "These lexicons are typically created either (a) manually, through crowdsourced annotations, or (b) automatically (e.g., calculating co-ocurrence of words with each emotion word). The version we'll be using was created through crowdsourcing -- details can be found in the [paper](https://arxiv.org/pdf/1308.6297.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1fEKfvstTOG"
      },
      "source": [
        "Let's first load in the lexicon and get a sense of what it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Br04QmOubrab"
      },
      "outputs": [],
      "source": [
        "from ccc_toolkit_v_21_0 import parse_emolex\n",
        "\n",
        "lexicon_path = 'NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
        "word2sentiments, word2emotions = parse_emolex(lexicon_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gZG3_bwtYCi"
      },
      "source": [
        "Here are some labeled positive/negative sentiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S9j0wqYej2dT",
        "outputId": "29125c81-1c62-4c9e-f367-df1d04731dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment lexicon:\n",
            "\n",
            "abandon {'negative'}\n",
            "abandoned {'negative'}\n",
            "abandonment {'negative'}\n",
            "abba {'positive'}\n",
            "abduction {'negative'}\n",
            "aberrant {'negative'}\n",
            "aberration {'negative'}\n",
            "abhor {'negative'}\n",
            "abhorrent {'negative'}\n",
            "ability {'positive'}\n",
            "abject {'negative'}\n"
          ]
        }
      ],
      "source": [
        "print('Sentiment lexicon:\\n')\n",
        "for i, (word, sentiments) in enumerate(word2sentiments.items()):\n",
        "    print(word, sentiments)\n",
        "    if i == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_DuZ6rttb-a"
      },
      "source": [
        "Here are some labeled emotions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BsSe-vYlkK0y",
        "outputId": "eb503912-3a0f-4113-bbfe-f85ce7c57d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion lexicon:\n",
            "\n",
            "abacus {'trust'}\n",
            "abandon {'fear', 'sadness'}\n",
            "abandoned {'anger', 'fear', 'sadness'}\n",
            "abandonment {'surprise', 'anger', 'fear', 'sadness'}\n",
            "abbot {'trust'}\n",
            "abduction {'surprise', 'fear', 'sadness'}\n",
            "aberration {'disgust'}\n",
            "abhor {'anger', 'fear', 'disgust'}\n",
            "abhorrent {'anger', 'fear', 'disgust'}\n",
            "abject {'disgust'}\n",
            "abnormal {'disgust'}\n",
            "abolish {'anger'}\n",
            "abominable {'disgust', 'fear'}\n",
            "abomination {'anger', 'fear', 'disgust'}\n",
            "abortion {'disgust', 'fear', 'sadness'}\n",
            "abortive {'sadness'}\n",
            "abrupt {'surprise'}\n",
            "abscess {'sadness'}\n",
            "absence {'fear', 'sadness'}\n",
            "absent {'sadness'}\n",
            "absentee {'sadness'}\n",
            "absolution {'joy', 'trust'}\n",
            "abundance {'joy', 'trust', 'anticipation', 'disgust'}\n",
            "abundant {'joy'}\n",
            "abuse {'sadness', 'anger', 'fear', 'disgust'}\n",
            "abysmal {'sadness'}\n",
            "abyss {'fear', 'sadness'}\n",
            "academic {'trust'}\n",
            "accelerate {'anticipation'}\n",
            "accident {'surprise', 'fear', 'sadness'}\n",
            "accidental {'surprise', 'fear'}\n"
          ]
        }
      ],
      "source": [
        "print('Emotion lexicon:\\n')\n",
        "for i, (word, emotions) in enumerate(word2emotions.items()):\n",
        "    print(word, emotions)\n",
        "    if i == 30:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyOc4sOMkP9Q"
      },
      "source": [
        "**Next, let's load load some comments and create a classifer with our lexicons.** Our approach is simple: a comment $x$ composed of a sequence of words $[w_0, w_1, ..., w_n]$, is classified as positive/negative or containing an emotion $e$ if any of the words $w_i$ is present in the lexicon. For example, a comment \"it was abnormal\" would be classified as containing the emotion \"digust\" because abnormal is mapped to disgust in the emotion lexicon.\n",
        "\n",
        "**Let's look at an example below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E3hSDQofkMqg",
        "outputId": "30546244-2ca7-460f-85b7-0222112fb814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comment: The way the current system is design has so many\n",
            "pitfalls, and it's all your fault.\n",
            "\n",
            "Words in utterance tagged with emotions:\n",
            "\n",
            "\t system {'trust'}\n",
            "\t fault {'sadness'}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "def get_paragraph(string, words=10):\n",
        "    return '\\n'.join([' '.join(x) for x in chunks(string.split(), words)])\n",
        "\n",
        "comment = \"\"\"\n",
        "The way the current system is design has so many pitfalls, and it's all your fault.\n",
        "\"\"\"\n",
        "\n",
        "print(f'comment: {get_paragraph(comment)}\\n')\n",
        "print('Words in utterance tagged with emotions:\\n')\n",
        "word_list = re.sub(\"[^\\w]\", \" \",  comment).lower().split()\n",
        "\n",
        "for word in word_list:\n",
        "    if word in word2emotions:\n",
        "        print('\\t', word, word2emotions[word])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeQGGZ75oUgT"
      },
      "source": [
        "This approach has some shortcomings! One issue is the binary categorization of words as positive or negative.\n",
        "\n",
        "To overcome this, we will try a slightly different approach.\n",
        "\n",
        "**We'll use a more advanced lexicon-based (and rule-based) method called VADER (Valence Aware Dictionary and sEntiment Reasoner). VADER consists of (1) a crowd-sourced valence-aware sentiment lexicon (i.e., each term has a *strength* associated with it), and 5 rules to incorporate features such as word-order sensitive relationships between terms.** These include intensifying valence through punctuation or capitlization, degree modifiers through adverbs, contrastive conjunction as a mixed signal with the latter clause dominating (e.g., \"The food here is great, but the service is horrible.\"), and negation of sentiment by preceding terms. The final sentiment is ultimately a sum of each word's valence and incorporation of these rules. See the [paper](http://eegilbert.org/papers/icwsm14.vader.hutto.pdf) and [repository of code](https://github.com/cjhutto/vaderSentiment) for more details.\n",
        "\n",
        "Though we are using it on conversation data, VADER also works well for social media posts, as it contains slang,\n",
        "commonly misspelled words, and emoticons in its lexicon (though some of these may be outdated now, as VADER came out in 2014)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dFwIci4fleQK",
        "outputId": "760a80d3-bc48-47c3-fa59-e21d910061df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vaderSentiment) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (4.0.0)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.columns"
      ],
      "metadata": {
        "id": "Q25JgjmuJRWe",
        "outputId": "a1af4f16-a60d-48be-e5b6-36dd3b4192ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'subreddit', 'subreddit description', 'violation reason',\n",
              "       'moderator comment', 'comment link', 'internal id'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MAtC07Q1odnR"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "comment_valences = []\n",
        "for i, comment in enumerate(reddit_data.text):\n",
        "    comment_valences.append(analyzer.polarity_scores(comment))\n",
        "reddit_data['vader_sentiment'] = comment_valences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb08IH00uFP-"
      },
      "source": [
        "Let's see what VADER tells us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2Ou9xObQpSIS",
        "outputId": "da5f307a-cc00-41f8-e432-510af0df1d27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, himbs is so sad that I won't agree to\n",
            "his weird need to control other people. Sad bro, real\n",
            "sad. I picture you at a party, lecturing at some\n",
            "hot thing as they frantically search for a way to\n",
            "escape the awful drudgery of listening to you.\n",
            ">> {'neg': 0.309, 'neu': 0.621, 'pos': 0.07, 'compound': -0.9325}\n",
            "\n",
            "\n",
            "Apparently his legs are too short to walk him back\n",
            "to the counter because he is a (baby). (Hopefully I\n",
            "won’t get banned for that) Honestly, WHY CANT HE ASK\n",
            "LIKE AN ADULT IF HE WANTS SOMETHING?? And then he\n",
            "doubles down saying he shouldn’t ‘have to’ act like an\n",
            "adult because he should be able to take anything he\n",
            "wants from her?! WHAT?!\n",
            ">> {'neg': 0.076, 'neu': 0.793, 'pos': 0.131, 'compound': 0.6239}\n",
            "\n",
            "\n",
            "Are you stupid? Why is this every ones solution on\n",
            "this site? Break up because he doesn’t clean as much??\n",
            "There’s such thing as communication, they don’t need to immediately\n",
            "jump the guns and break up.\n",
            ">> {'neg': 0.08, 'neu': 0.779, 'pos': 0.141, 'compound': 0.3736}\n",
            "\n",
            "\n",
            "You’re an absolutely disgusting, disturbed fucking b****. In what world\n",
            "is leaving a defenseless, crying baby in a soiled diaper\n",
            "the best option, even if the mother shouldn’t have left\n",
            "them. Worms for brains, and the fact that you constantly\n",
            "referred to them as “it”? Weirdo behavior. There’s no way\n",
            "you could have spun this to make your behavior acceptable,\n",
            "freak. YTA.\n",
            ">> {'neg': 0.251, 'neu': 0.667, 'pos': 0.082, 'compound': -0.9095}\n",
            "\n",
            "\n",
            "Stealing. This isn’t about how you were treated. It’s about\n",
            "you stealing. Committing a crime. You are delusional because you\n",
            "are focused on feeling justified on committing a crime because\n",
            "you were inconvenienced. If you are going to continue to\n",
            "be a dumb fuck don’t reply.\n",
            ">> {'neg': 0.323, 'neu': 0.534, 'pos': 0.143, 'compound': -0.9413}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (_, row) in enumerate(reddit_data.iterrows()):\n",
        "    if i == 5: break\n",
        "    print(get_paragraph(row.text))\n",
        "    print('>>', row.vader_sentiment)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZMrYiLopoJd"
      },
      "source": [
        "The `compound` score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A brief ending note on classifers\n",
        "\n",
        "We used lexicon- and rule-based classifiers in our labs today. Another approach are *feature-based* classifiers. In traditional machine learning models, a *feature extraction* step is first applied to an input text, extracting up to hundreds of lexical features about the words present, parts of speech present, ordering features, lexicon-based features, etc. Assuming a labeled dataset $X,Y$, where $X$ are the input texts and $Y$ are the ground-truth labels, the model learns probabilistically to predict a label $y$ given the extracted features of $x$.\n",
        "\n",
        "In deep neural networks (DNNs), this feature extraction step is skipped. Instead of having to hand-engineer features, DNNs can automatically learn features that are helpful for predicting $y$. Typically, these automatically-derived features will include the hand-engineered features -- see [1](https://www.aclweb.org/anthology/N19-1419.pdf), [2](https://hal.inria.fr/hal-02131630/document), etc.\n",
        "\n",
        "Deep neural networks and other machine learning methods now typically produce better results than lexicon-based approaches. However, a lexicon-based approach can still be useful, as shown with VADER, and be (a) a strong baseline, (b) useful for producing weak labels to train a machine learning model, and (c) act as a sanity check for a machine learning model (e.g., a neural-network based toxicity classifier such as [Perspective API](https://www.perspectiveapi.com/#/home) correlates extremely strongly with a simple expletive-lexicon-based classifier on Facebook comments.)"
      ],
      "metadata": {
        "id": "Q0BvVrHkm5iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Perspective API"
      ],
      "metadata": {
        "id": "IkmBuWYSeF7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Perspective is a free API that helps you host better conversations online.\n",
        "The API uses machine learning models to score the perceived impact a comment\n",
        "might have on a conversation. You can use this score to give feedback to\n",
        "commenters, help moderators more easily review comments, allow readers\n",
        "to more easily find interesting or productive comments, and more.\"\n",
        "\n",
        "https://developers.perspectiveapi.com/s/?language=en_US"
      ],
      "metadata": {
        "id": "9yNWo7TIenDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey='AIzaSyBMQ_87KcUcRFTHlEus8JYxzuy7wBAVaI0',\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "text = reddit_data.text[0]\n",
        "print(text, '\\n\\n------')\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': text },\n",
        "  'requestedAttributes': {'TOXICITY': {},\n",
        "                          'THREAT': {},\n",
        "                          'INSULT': {},\n",
        "                          'IDENTITY_ATTACK': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(json.dumps(response, indent=2))"
      ],
      "metadata": {
        "id": "11iZAhDicD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9zKKphYcD2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEe2JxVZphRa"
      },
      "source": [
        "# 4. Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-m-17Wfp9PU"
      },
      "source": [
        "#### Load models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfMKzoRcHpUJ"
      },
      "source": [
        "Again, this may take a minute or two. Head on over to the next section while you're waiting for this to finish!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RFd77gwkFF3"
      },
      "source": [
        "# https://nlp.stanford.edu/projects/glove/\n",
        "# https://github.com/RaRe-Technologies/gensim-data\n",
        "\n",
        "import gensim.downloader as api\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\") # \"glove-twitter-50\"\n",
        "glove_model_vocab = glove_model.wv.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbyP0GIbJtfW"
      },
      "source": [
        "### You shall know a word by the company it keeps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dqnsoMuDAI"
      },
      "source": [
        "A starting question in natural language processing is how to represent words and text. As we saw previously, one simple approach is called \"bag of words\" (BoW), which simply counts the number of times each word appears. In this approach, words are represented as a \"one-hot vector\", corresponding to a vector of 0's and a 1 for the index of that word. For example, if our vocabulary consists of `['drink', 'eat' 'pray', 'love', 'earthquake']`, then `drink = [1,0,0,0,0]` and `love = [0,0,0,1,0]`.\n",
        "\n",
        "--- \n",
        "\n",
        "While simple, one drawback to this approach is that semantically related words don't have similar representations. For instance, we might want \"eat\" and \"drink\" to be more similar to each other than \"eat\" and \"earthquake\". In order to measure similarity, we must first have a distance metric. One common distance metric is Euclidean distance, defined between two vectors $p$ and $q$ as $d(p,q) = \\sqrt{\\sum_{i=1}^n (p_i - q_i) ^ 2}$. However, note that $d(eat,drink) = d(eat,earthquake)$. In fact, all of the words are equally similar using this one-hot representation.\n",
        "\n",
        "--- \n",
        "\n",
        "The goal is to thus learn representations that can better capture notions of semantic and lexical similarity. The **Word2vec** model is one popular such approach. Each word is initialized with a randomly distributed (typically Gaussian-like) vector representation. Models are trained on large corpora to learn which words frequently co-occur together. For instance, given the context `the hungry hippo ____ his meal`, the model learns to predict that `devoured` and `ate` are reasonable words. Over the course of training, these vector representations are improved such that (1) they can better predict neighboring words, and, as a natural byproduct, (2) they better capture the semantic similarity we desired.\n",
        "\n",
        "---\n",
        "\n",
        "Let's take a look at what exactly we mean by this vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMBg_Y3Dxylp"
      },
      "source": [
        "print('Each word is represented as a {}-dimensional vector'.format(\n",
        "    glove_model.wv['orange'].shape[0]))\n",
        "print(glove_model.wv['orange'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvNQJdzUJ4-v"
      },
      "source": [
        "### Semantic neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDB2fsTra5xp"
      },
      "source": [
        "Let's look at what we can do once we have these vector representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88y8c4g1WLJ5"
      },
      "source": [
        "**First, a sanity check. Let's compute similarity scores between pairs of words.** The default distance metric is cosine similarity, which accounts for the *direction* of the vectors while normalizing for the *magnitude*. This score ranges from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBAiqw8COIFW"
      },
      "source": [
        "def compute_similarity(pairs, model):\n",
        "    \"\"\"\n",
        "    This is a helper function to print the similarity of each\n",
        "    pair of words in pairs. The output is printed in sorted order\n",
        "    of similarity.\n",
        "\n",
        "    Args:\n",
        "        pairs: list of tuples, each tuple contains two strings\n",
        "        model: Gensim word2vec model with a similarity() function\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for w1, w2 in pairs:\n",
        "        sim = model.similarity(w1, w2)\n",
        "        results.append((w1, w2, sim))\n",
        "\n",
        "    for w1, w2, sim in sorted(results, key=lambda x: -x[2]):\n",
        "        # print('%r\\t%r\\t%.2f' % (w1, w2, sim))\n",
        "        print('{:>12}\\t{:>12}\\t{:.2f}'.format(w1, w2, sim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx4hF4o4n-KC"
      },
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "\n",
        "compute_similarity(pairs, glove_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7Tzm4fWEhu"
      },
      "source": [
        "**Perhaps you're interested in something a little trickier, such as the similarity of fruits, colors, or size adjectives.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnO4ZIWxOdF1"
      },
      "source": [
        "items = ['apple', 'cantaloupe', 'banana', 'coconut', 'pineapple', 'watermelon']\n",
        "# items = ['red', 'orange', 'yellow', 'blue', 'green', 'indigo', 'violet']\n",
        "# items = ['big', 'large', 'huge', 'enormous', 'gargantuan', 'vast']\n",
        "pairs = itertools.combinations(items, 2)  # creates pairwise combinations\n",
        "\n",
        "compute_similarity(pairs, glove_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ru-rTiPV10a"
      },
      "source": [
        "**We can also find a word's nearest neighbors.**\n",
        "\n",
        "For the word 'plum' (example below), these may include other fruits, specific plums (Kakadu plum), and also the notion of a \"plum\" job (sinecure, cushy, plum assignments, coveted). You may wonder why actor Yeager Lithgow apepars further down\n",
        "the list. A quick Google search surfaces a number of \n",
        "articles (on which this Word2vec model was trained) about\n",
        "\"[his wife] pushing him into the plum job\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSRknAguK7nC"
      },
      "source": [
        "result = glove_model.most_similar(positive=['plum'], topn=15)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7KKZnIMWWix"
      },
      "source": [
        "**Remember that word embeddings are trained by learning which words co-occur with other words. This means that the *.most_similar()* function doesn't necessariliy return words with the same *meaning* -- instead, it returns words that frequently occur with that word.**\n",
        "\n",
        "For example, while most of the most similar words for 'happy' are positive, 'disappointed' and even *'unhappy'* are within the top 15 matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0gHVPXHRycC"
      },
      "source": [
        "result = glove_model.most_similar(positive=['happy'], topn=20)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcefSRMrW89A"
      },
      "source": [
        "**We can also find words that are similar to *multiple* words.** This corresponds to finding words in vector space near the average of the two words (or centroid if multiple words are given)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kchrXpqW8CA"
      },
      "source": [
        "result = glove_model.most_similar(positive=['french', 'pastry'], topn=5)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSCeTdTMK945"
      },
      "source": [
        "**Word embeddings are also famous for being able to reconstruct analogies of the form `A is to B as C is to ___`.** For example, if we calculate `king + woman - man`, the closest word embedding is `queen`! (There are some [nuances](https://www.facebook.com/groups/1174547215919768/permalink/1846673885373761/)). \n",
        "\n",
        "Let's try it out for ourselves. In the following, we're finding the terms most similar to both woman and king, but dissimilar from man."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY1PSEBsoEks"
      },
      "source": [
        "print('King + woman - man:')\n",
        "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n",
        "pprint(result)\n",
        "\n",
        "print('\\n' + '-' * 100 + '\\n')\n",
        "\n",
        "\n",
        "# You'll see that warmer appears as a top term.\n",
        "# This is due to the co-ocurrence nature we mentioned before, where\n",
        "# colder and warmer are likely to occur in similar contexts, and hence\n",
        "# have similar vector representations. \n",
        "print('Louder + cold - loud:')\n",
        "result = glove_model.most_similar(positive=['louder', 'cold'], negative=['loud'], topn=3)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvlUbLPQbAMi"
      },
      "source": [
        "**Finally, note that word embeddings may contain [\"human-like\" biases](https://science.sciencemag.org/content/356/6334/183.abstract)**. For example, if we compute `computer_programmer + woman - man`, the top result is `homemaker`. See for yourself below.\n",
        "\n",
        "These biases can have harmful repercussions on downstream tasks. There are [methods to debias](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf) these embeddings, but there are [limitations](https://arxiv.org/pdf/1903.03862.pdf) to these methods as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_4lf3-va_di"
      },
      "source": [
        "print('Doctor + woman - man:')\n",
        "result = glove_model.most_similar(positive=['doctor', 'woman'],\n",
        "                                   negative=['man'], topn=3)\n",
        "pprint(result)\n",
        "\n",
        "print('\\n' + '-' * 100 + '\\n')\n",
        "\n",
        "print('Computer + woman - man:')\n",
        "result = glove_model.most_similar(positive=['computer', 'woman'],\n",
        "                                   negative=['man'], topn=3)\n",
        "pprint(result)\n",
        "\n",
        "print('\\n' + '-' * 100 + '\\n')\n",
        "print('Similarity between \"criminal\" and various names')\n",
        "pairs = [\n",
        "    ('criminal', 'matthew'),\n",
        "    ('criminal', 'bob'),\n",
        "    ('criminal', 'jake'),\n",
        "    ('criminal', 'darnell'),\n",
        "    ('criminal', 'trayvon'),\n",
        "    ('criminal', 'deshawn'),\n",
        "    ('criminal', 'alexander'),\n",
        "    ('criminal', 'aleksander'),\n",
        "    ('criminal', 'camilo'),\n",
        "    ('criminal', 'belén'),\n",
        "]\n",
        "compute_similarity(pairs, glove_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q89I0RYz4pLe"
      },
      "source": [
        "# 5. Embedding-based text retrieval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGXBR7T8KDSJ"
      },
      "source": [
        "When we come upon a particularly interesting review, we might be interested in retrieving text from all conversations that matches that topic.\n",
        "\n",
        "One way we can do this is through a \"text retrieval\" approach. \n",
        "\n",
        "Our problem setup is: given a query $q$ (\"cancelled flight\") and a set of documents $D$ (our set of comments), we want to retrieve (and rank) the $n$ documents that most closely match our query. A query can be any natural language string (e.g. a word, a phrase, a sentence, a paragraph, etc.). Each document can similarly be any natural language string.\n",
        "\n",
        "We will be using a model called [Sentence-BERT](https://arxiv.org/abs/1908.10084), which computes a high dimensional vector representation of each sentence. It's an exciting method that would deserve its own workshop!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx_r0BATaEVp"
      },
      "source": [
        "**Let's first load the model and the data.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBpgE12trH0C"
      },
      "outputs": [],
      "source": [
        "# There are different versions of the model, specified by the name 'bert-base...'\n",
        "\n",
        "sent_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kgsq26KwMiY"
      },
      "outputs": [],
      "source": [
        "# we are only using a subset of LVN public data.\n",
        "# with open('lvn_subset_data.pickle', 'rb') as handle:\n",
        "#    lvn_data = pickle.load(handle)\n",
        "import numpy as np\n",
        "np.random.seed(111)\n",
        "\n",
        "text = reddit_data.text.values.tolist()\n",
        "\n",
        "# alternatively, we can run BERT model to get embeddings, which takes ~30 mins.\n",
        "comments_embeddings = sent_model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "print(\"Sampled data contains {} comments.\".format(len(text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dauy1zZdo-pm"
      },
      "source": [
        "What does an \"embedding\" representation look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIr8rORgrguP"
      },
      "outputs": [],
      "source": [
        "comments_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID9Xya3janX4"
      },
      "source": [
        "**Next, let's define the retrieval function and perform some searches.** For each query, we embed it using the same SentenceModel we used to embed the utterances. This means that both queries and utterances now live in the same high-dimensional vector space (to some degree...). We can now compute the cosine similarity between a query and each utterance to retrieve the most similar utterances in LVN conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN7Yq8WNSvFW"
      },
      "outputs": [],
      "source": [
        "queries = [\"it's your fault. You insulted her.\"]\n",
        "\n",
        "retrieve(sent_model, queries, text, comments_embeddings, closest_n=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCdoDwAbbWz2"
      },
      "source": [
        "**Great. Try creating some of your own queries.** Some questions you may be interested in asking include:\n",
        "- Do queries that are paraphrases of each other return similar matches?\n",
        "- For a query of your choosing, how does the average similarity score (over the top n results) compare between queries? What does this say about the relevance of that query / topic?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0c8OF9Sbbfw"
      },
      "outputs": [],
      "source": [
        "# Enter your own query between the quotation marks of your_query below!\n",
        "your_query = [\"\"\"My neighbour who sucks at farming but is good at\n",
        "making wooden wheels makes a deal with me. I will work on his farm,\n",
        "give him a share of the crops every harvest, and in return I get\n",
        "to sell the other portion. Is that slavery? How is that possibly slavery?\"\"\"]\n",
        "retrieve(sent_model, your_query, text, comments_embeddings, closest_n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg3dV8CNp48g"
      },
      "source": [
        "# 6. Clustering and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjiqWHgpY4h"
      },
      "source": [
        "From the activity above, we see there is some overlap in the topics/sentiment that people bring up across comments.\n",
        "\n",
        "Assuming we don't want to think of and retrieve each topic individually, how might we sort and visualize our entire set of conversations?\n",
        "\n",
        "One approach would be through **unsupervised clustering.** A popular method for this is called K-means clustering. Let's watch a short video about K-means below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6szkarzv4KK"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!--\n",
        "\"\"\"\n",
        "Here the algorithm working on a dataset of 300 \"documents\" embedded to\n",
        "two dimensions.\n",
        "\"\"\"\n",
        "-->\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5I3Ei69I40s\"\n",
        "frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\"\n",
        "allowfullscreen>\n",
        "</iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSElL0Gfpy4k"
      },
      "source": [
        "Next, let's implement K-means clustering and visualize its results in 2-dimensional space.\n",
        "\n",
        "K-means clusters data points in high dimensions.\n",
        "- Originally we have **768** dimensions from **BERT**.\n",
        "- To run **k-mean** we need to compress that space to a lower dimensionality one. We do that by using **PCA** to capture as much linear variance (information) as possible.\n",
        "- We run **k-mean** on those lower-dimensionality vectors.\n",
        "- Finally, we reduce dimensionality to 2D using **t-SNE** for visualization purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvDOON2Kq3WZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Every time you run this cell you'll get different clusters and embeddings\n",
        "due to the fact that these methods calculate approximate transformations\n",
        "that depend on the initial random seed.\n",
        "\"\"\"\n",
        "set_replicable_results(True)\n",
        "\n",
        "viz_coord, clstr_model, predicted_clusters, _, _ = run_clustering(text,\n",
        "                                                                  pca_components=20,\n",
        "                                                                  k_clusters=20,\n",
        "                                                                  embeddings=comments_embeddings.cpu())\n",
        "\n",
        "# hover the dots in the plot with your mouse\n",
        "# to see where each piece of text got located.\n",
        "\n",
        "plot_tsne_viz(viz_coord, text, title=\"T-SNE<br>Hover the dots in the plot with your mouse.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_G3OzvKyHpO"
      },
      "source": [
        "It's hard to make sense of this. There aren't clear clusters, and it doesn't tell us anything important without additional manual labor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suEy5IzQrd_m"
      },
      "source": [
        "Let's color our 2D visualization according to the cluster ids we found through k-means. How does it look?\n",
        "\n",
        "Do we see any clustering that we would expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1lyM3UUqCHz"
      },
      "outputs": [],
      "source": [
        "# Predicted clusters\n",
        "plot_tsne_viz(viz_coord, text,\n",
        "                  clusters = predicted_clusters, \n",
        "                  coloring='clusters',\n",
        "                  title=\"K-means clusters. We ran k-means and got the following clusters:\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "E6rjRQDH93cP",
        "hWXr0TbU_oGF",
        "GzVdhct0tzSN",
        "Baxg7zEObPVk",
        "tgLTES_uQeUl",
        "YxRb97zCboQo",
        "Q0BvVrHkm5iu",
        "IkmBuWYSeF7T",
        "xEe2JxVZphRa",
        "r-m-17Wfp9PU",
        "WbyP0GIbJtfW",
        "dvNQJdzUJ4-v",
        "Q89I0RYz4pLe",
        "tg3dV8CNp48g"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}